uirements
- Python 3.10+
- dbt-core installed
- Create a bucket on Google cloud storage (and maintain the region credentials for the set up for bigquery)
- BigQuery credentials set up (if using GCP)
- Set up your dbt/profiles.yml file such that all needed configurations are properly linked to your project ID, dataset, table ,target database where all transformed data would be stored and the key which would be the absolute path to where you store the service account.
  

## Setup
running the kestra scripts with the set-up script first and then the extraction and loading script last does the rest of the job, just ensure the actual credentials are copied and replaced in the kestra files.

## run dbt
a service account json file needs to be generated by you from the gcp console and stored anywhere of your choice, preferably outside of your working directory and the absolute path needs to be filled as the value to your "key" in your profiles.yml file.

Afterwards, run a 'dbt run', all models would get created and the dataset populated.
